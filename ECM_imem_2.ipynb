{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "_Iv7Y3cpiRbv",
        "5uhFnwA4iRbv",
        "tszKJRFZiRcA",
        "w2KC7N9ydYlV",
        "hU4mq3qSlwg_",
        "Gg6M75fPLjfJ",
        "eyzlMiuC6BEN",
        "0MJ_JNy96PjG",
        "f4BuAXU36tR2",
        "Sf0kqimNdaYD"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "snaUD66uv15a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b94b9e82-849d-49b9-a207-19d4296a387b"
      },
      "cell_type": "code",
      "source": [
        "!pip install contractions\n",
        "import contractions"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.6/dist-packages (0.0.18)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-09T18:34:06.340440Z",
          "start_time": "2019-04-09T18:34:05.777470Z"
        },
        "id": "8ru_RVTyiRbn",
        "colab_type": "code",
        "outputId": "9ba2c987-c80d-42f3-b619-df9fd0cecc48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import torch\n",
        "from torch.jit import script, trace\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import operator\n",
        "import codecs\n",
        "from io import open\n",
        "import itertools\n",
        "import math\n",
        "import pickle\n",
        "import numpy as np\n",
        "from queue import PriorityQueue\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = torch.device(\"cpu\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_Iv7Y3cpiRbv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load and Preprocess Data"
      ]
    },
    {
      "metadata": {
        "id": "5uhFnwA4iRbv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Class Voc\n",
        "\n",
        "Preprocess text data to prepare it for training"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-09T18:36:36.530462Z",
          "start_time": "2019-04-09T18:36:36.452493Z"
        },
        "id": "nD5_gejJiRbw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Default word tokens\n",
        "PAD_token = 0  # Used for padding short sentences\n",
        "SOS_token = 1  # Start-of-sentence token\n",
        "EOS_token = 2  # End-of-sentence token\n",
        "num_emotion = 5\n",
        "emo2idx = {'neutral':0,'joy':1,'sadness':2,'fear':3,'anger':4}\n",
        "idx2emo = {0:'neutral',1:'joy',2:'sadness',3:'fear',4:'anger'}\n",
        "MAX_LENGTH = 20\n",
        "\n",
        "\n",
        "class Voc:\n",
        "    def __init__(self,name,version,normalize=False):\n",
        "        self.name = name\n",
        "        self.version = version\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3  # Count SOS, EOS, PAD\n",
        "        self.normalize = normalize\n",
        "        if version == \"pre-trained-word2vec-100d\" or version=='pre-trained-word2vec-300d':\n",
        "            from torchnlp.word_to_vector import GloVe\n",
        "            if version == \"pre-trained-word2vec-300d\":\n",
        "                self.dim = 300 \n",
        "                self.glove = GloVe()\n",
        "                self.weights_matrix = np.zeros((10000,self.dim))\n",
        "            else:\n",
        "                self.dim = 100 \n",
        "                self.glove = GloVe(name='6B', dim=self.dim)\n",
        "            self.weights_matrix = np.zeros((10000,self.dim))\n",
        "            self.weights_matrix[0] = self.glove[str(PAD_token)]\n",
        "            self.weights_matrix[1] = self.glove[str(SOS_token)]\n",
        "            self.weights_matrix[2] = self.glove[str(EOS_token)]\n",
        "        elif version == \"pre-trained-subword-100d\":\n",
        "            from bpemb import BPEmb\n",
        "            self.dim = 100\n",
        "            self.bpemb = BPEmb(lang=\"en\", dim=self.dim)\n",
        "            self.index2word = {PAD_token: self.bpemb.decode_ids([PAD_token]), \n",
        "                               SOS_token: self.bpemb.decode_ids([SOS_token]), \n",
        "                               EOS_token: self.bpemb.decode_ids([EOS_token])} \n",
        "            self.weights_matrix = self.bpemb.vectors.copy()\n",
        "    \n",
        "    # Load the data to go through the pipeline\n",
        "    def loadData(self, filepath,small=False):\n",
        "        print(\"Start preparing data ...\")\n",
        "        normalized_pairs = []\n",
        "        lines = open(datafile, encoding='utf-8').read().strip().split('\\n')\n",
        "        if small:\n",
        "            lines = lines[:1000]\n",
        "        for i,line in enumerate(lines):\n",
        "            pair = line.split('\\t')\n",
        "            post = self.addSentence(pair[0].strip())\n",
        "            response = self.addSentence(pair[1].strip())\n",
        "            if(post and response):\n",
        "                normalized_pairs.append([post,response])\n",
        "                if i%10000 == 0:\n",
        "                    print('Loading {}th data pairs'.format(i))\n",
        "                    print(post,response)\n",
        "        return normalized_pairs\n",
        "      \n",
        "    def loadDataFromPickle(self, filepath,small=False):\n",
        "        print(\"Start preparing data ...\")\n",
        "        train_set,val_set,test_set  = [],[],[]\n",
        "        train_emo,val_emo,test_emo  = [],[],[]\n",
        "        \n",
        "        f = open(filepath+'/train.pickle', 'rb')\n",
        "        train_data,train_emo_set = pickle.load(f),pickle.load(f)\n",
        "        for i,line in enumerate(train_data):\n",
        "            post = self.addSentence(line[0].strip())\n",
        "            response = self.addSentence(line[1].strip())\n",
        "            if len(post.split(' '))< 5 or len(response.split(' ')) < 3:\n",
        "                continue\n",
        "            train_set.append([post,response])\n",
        "            train_emo.append(train_emo_set[i])\n",
        "            if (i%10000 == 0):\n",
        "                print('loading {} th pair from training dataset'.format(i),post,response)\n",
        "            if small and i == 10000:\n",
        "                break\n",
        "        \n",
        "        f = open(filepath+'/val.pickle', 'rb')\n",
        "        val_data,val_emo_set = pickle.load(f),pickle.load(f)\n",
        "        for i,line in enumerate(val_data):\n",
        "            post = self.addSentence(line[0].strip())\n",
        "            response = self.addSentence(line[1].strip()) \n",
        "            if len(post.split(' '))< 5 or len(response.split(' ')) < 3:\n",
        "                continue\n",
        "            val_set.append([post,response])\n",
        "            val_emo.append(val_emo_set[i])\n",
        "            if (i%2000 == 0):\n",
        "                print('loading {} th pair from validation dataset'.format(i),post,response)\n",
        "        \n",
        "        \n",
        "        f = open(filepath+'/test.pickle', 'rb')\n",
        "        test_data,test_emo_set = pickle.load(f),pickle.load(f)\n",
        "        for i,line in enumerate(test_data):\n",
        "            post = self.addSentence(line[0].strip())\n",
        "            response = self.addSentence(line[1].strip())\n",
        "            if len(post.split(' '))< 5 or len(response.split(' ')) < 3:\n",
        "                continue\n",
        "            test_set.append([post,response])\n",
        "            test_emo.append(test_emo_set[i])\n",
        "            if (i%2000 == 0):\n",
        "                print('loading {} th pair from training dataset'.format(i),post,response)\n",
        "        \n",
        "        return train_set,train_emo,val_set,val_emo,test_set,test_emo\n",
        "      \n",
        "      \n",
        "    # Batch [post, response] pair to required training format  \n",
        "    def batchSent2VecData(self,idxs,pairs,emos=None):\n",
        "        pair_batch = [pairs[i] for i in idxs]\n",
        "        if emos != None:\n",
        "            emo_batch = [emos[i] for i in idxs]\n",
        "        pair_batch.sort(key=lambda x: len(x[0].split(' ')), reverse=True)\n",
        "        input_batch,output_batch,emo_in,emo_out = [], [], [], []\n",
        "        for i,pair in enumerate(pair_batch):\n",
        "            input_batch.append(pair[0])\n",
        "            output_batch.append(pair[1])\n",
        "            if emos!=None:\n",
        "                emo_in.append(emo2idx[emo_batch[i][0]])\n",
        "                emo_out.append(emo2idx[emo_batch[i][1]])\n",
        "        inp, lengths = self.inputVar(input_batch)\n",
        "        output, mask, max_target_len = self.outputVar(output_batch)\n",
        "        \n",
        "        return inp,lengths,output,mask, max_target_len,torch.LongTensor(emo_in),torch.LongTensor(emo_out)\n",
        "    \n",
        "\n",
        "    # Helper methods to facilitate the functions above\n",
        "    def tokenizer(self,s,normalize=False):\n",
        "        if (normalize):\n",
        "            s = ''.join(c for c in unicodedata.normalize('NFD', s.lower().strip())\n",
        "                if unicodedata.category(c) != 'Mn')\n",
        "            s = contractions.fix(s)\n",
        "            s = re.sub(r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\;]\", \" \", s)\n",
        "            s = re.sub(r\"[ ]+\", \" \", s)\n",
        "            s = re.sub(r\"\\!+\", \"!\", s)\n",
        "            s = re.sub(r\"\\,+\", \",\", s)\n",
        "            s = re.sub(r\"\\?+\", \"?\", s)\n",
        "        if self.version  == \"pre-trained-subword-100d\":\n",
        "            return self.bpemb.encode(s)\n",
        "        else: \n",
        "            return s.strip().split(' ')\n",
        "          \n",
        "    def addSentence(self, sentence,normalize=False):\n",
        "        words = self.tokenizer(sentence,normalize)\n",
        "        words = words[:min(MAX_LENGTH,len(words))]\n",
        "        for word in words:\n",
        "            self.addWord(word)\n",
        "        return ' '.join(words)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if not word:\n",
        "            return\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "            if self.version in [\"pre-trained-word2vec-100d\",\"pre-trained-word2vec-300d\"]:\n",
        "                self.weights_matrix[self.num_words] = self.glove[word]\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "    \n",
        "    def getWordIndex(self,word):\n",
        "        return self.word2index[word]\n",
        "    \n",
        "\n",
        "    def indexesFromSentence(self,sentence,normalize=False):\n",
        "        if self.version == \"pre-trained-subword-100d\":\n",
        "            return self.bpemb.encode_ids(sentence) + [EOS_token]\n",
        "        return [self.getWordIndex(word) for word in self.tokenizer(sentence,normalize)] + [EOS_token]\n",
        "\n",
        "    def binaryMatrix(self,l, value=PAD_token):\n",
        "        m = []\n",
        "        for i, seq in enumerate(l):\n",
        "            m.append([])\n",
        "            for token in seq:\n",
        "                if token == PAD_token:\n",
        "                    m[i].append(0)\n",
        "                else:\n",
        "                    m[i].append(1)\n",
        "        return m\n",
        "    def zeroPadding(self,l,fillvalue=PAD_token):\n",
        "        return list(itertools.zip_longest(*l, fillvalue=fillvalue))  \n",
        "\n",
        "    def inputVar(self,l):\n",
        "        indexes_batch = [self.indexesFromSentence(sentence) for sentence in l]\n",
        "        lengths = torch.tensor([len(indexes) for indexes in indexes_batch],dtype=torch.int64)\n",
        "\n",
        "        padList = self.zeroPadding(indexes_batch)\n",
        "        padVar = torch.LongTensor(padList)\n",
        "        return padVar, lengths\n",
        "      \n",
        "    # Returns padded target sequence tensor, padding mask, and max target length\n",
        "    def outputVar(self,l):\n",
        "        indexes_batch = [self.indexesFromSentence(sentence) for sentence in l]\n",
        "        max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
        "        padList = self.zeroPadding(indexes_batch)\n",
        "        mask = self.binaryMatrix(padList)\n",
        "        mask = torch.ByteTensor(mask)\n",
        "        padVar = torch.LongTensor(padList)\n",
        "        return padVar,mask,max_target_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bmHD9gfeiRb9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model"
      ]
    },
    {
      "metadata": {
        "id": "qJLlMyj_iRb-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Encoder, Decoder, Attn"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-09T18:37:50.631806Z",
          "start_time": "2019-04-09T18:37:50.304497Z"
        },
        "id": "Q5i-RapsiRb-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, embedding, n_layers=1, dropout=0):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = embedding.weight.size()[1]\n",
        "        self.embedding = embedding\n",
        "\n",
        "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
        "        #   because our input size is a word embedding with number of features == hidden_size\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, n_layers,\n",
        "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
        "\n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "        # Convert word indexes to embeddings\n",
        "        embedded = self.embedding(input_seq)\n",
        "        # Pack padded batch of sequences for RNN module\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "        # Forward pass through GRU\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "        # Unpack padding\n",
        "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        # Sum bidirectional GRU outputs\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
        "        # Return output and final hidden state\n",
        "        return outputs, hidden\n",
        "# Luong attention layer\n",
        "\n",
        "class Attn(torch.nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        self.method = method\n",
        "        if self.method not in ['dot', 'general', 'concat']:\n",
        "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
        "        self.hidden_size = hidden_size\n",
        "        if self.method == 'general':\n",
        "            self.attn = torch.nn.Linear(self.hidden_size, hidden_size)\n",
        "        elif self.method == 'concat':\n",
        "            self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "#             self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))\n",
        "            self.v = torch.nn.Parameter(torch.randn(hidden_size,dtype=torch.float))\n",
        "\n",
        "    def dot_score(self, hidden, encoder_output):\n",
        "        return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "    def general_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(encoder_output)\n",
        "        return torch.sum(hidden * energy, dim=2)\n",
        "\n",
        "    def concat_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
        "        return torch.sum(self.v * energy, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # Calculate the attention weights (energies) based on the given method\n",
        "        if self.method == 'general':\n",
        "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'concat':\n",
        "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'dot':\n",
        "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
        "\n",
        "        # Transpose max_length and batch_size dimensions\n",
        "        attn_energies = attn_energies.t()\n",
        "        # Return the softmax normalized probability scores (with added dimension)\n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
        "      \n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self,attn_model,embedding,emotion_embedding,n_layers=1,dropout=0.1,use_emb=False,use_imemory=False,use_ememory=False):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        # Keep for reference\n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = embedding.weight.size()[1]\n",
        "        self.output_size = embedding.weight.size()[0]\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "        self.use_emb = use_emb\n",
        "        self.use_imemory = use_imemory\n",
        "        self.use_ememory = use_ememory\n",
        "\n",
        "        # Define layers\n",
        "        self.embedding = embedding\n",
        "        self.emotion_embedding = emotion_embedding\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
        "        self.concat_1 = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.concat_3 = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "        \n",
        "        \n",
        "        # DIY layers\n",
        "        self.read_linear = nn.Linear(self.hidden_size*(self.n_layers+1),self.hidden_size)\n",
        "        self.write_linear = nn.Linear(self.hidden_size,self.hidden_size)\n",
        "        self.gru_concat = nn.Linear(self.hidden_size*2,self.hidden_size)\n",
        "\n",
        "        self.attn = Attn(attn_model, self.hidden_size)\n",
        "        \n",
        "    def forward(self, input_step, emotion, last_hidden, encoder_outputs):\n",
        "        # Note: we run this one step (word) at a time\n",
        "        # Get embedding of current input word\n",
        "        embedded = self.embedding(input_step) # 1,64,300\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "        \n",
        "        if self.use_emb and self.use_imemory:\n",
        "          \n",
        "            if emotion.size()!=embedded.size():\n",
        "                emotion = self.emotion_embedding(emotion).unsqueeze(dim=0) # mem_write\n",
        "            \n",
        "            _,tmp_size,_ = last_hidden.size()\n",
        "\n",
        "            read_gate = torch.sigmoid(self.read_linear(torch.cat([embedded,torch.reshape(last_hidden,(1,tmp_size,-1))],dim=2)))\n",
        "            mem_read = torch.mul(emotion,read_gate)\n",
        "            \n",
        "            gru_input = self.gru_concat(torch.cat([embedded,mem_read],dim=2))\n",
        "            rnn_output, hidden = self.gru(gru_input, last_hidden)\n",
        "            \n",
        "            write_gate = torch.sigmoid(self.write_linear(rnn_output))\n",
        "            emotion = torch.mul(write_gate,mem_read)\n",
        "          \n",
        "          \n",
        "          \n",
        "\n",
        "            attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "            # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
        "            context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "            # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
        "            rnn_output = rnn_output.squeeze(0)\n",
        "            context = context.squeeze(1)\n",
        "            concat_input = torch.cat((rnn_output, context), 1)\n",
        "            concat_output = torch.tanh(self.concat_3(concat_input))\n",
        "            # Predict next word using Luong eq. 6\n",
        "            output = self.out(concat_output)\n",
        "            output = F.softmax(output, dim=1)\n",
        "            return output,hidden,emotion\n",
        "        elif self.use_emb:\n",
        "            emotion_embedded = self.emotion_embedding(emotion).unsqueeze(dim=0)\n",
        "            embedded = self.concat_1(torch.cat([embedded,emotion_embedded],dim = 2))\n",
        "\n",
        "        # Forward through unidirectional GRU\n",
        "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
        "        # Calculate attention weights from the current GRU output\n",
        "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat_3(concat_input))\n",
        "        # Predict next word using Luong eq. 6\n",
        "        output = self.out(concat_output)\n",
        "        output = F.softmax(output, dim=1)\n",
        "        # Return output and final hidden state\n",
        "        return output,hidden,emotion      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tszKJRFZiRcA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loss Function, Train, TrainIter Methods"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-09T18:38:06.407505Z",
          "start_time": "2019-04-09T18:38:06.266358Z"
        },
        "id": "xWNoxYpBiRcD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def maskNLLLoss(inp,emotion,target,mask,decoder):\n",
        "    nTotal = mask.sum()\n",
        "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "    loss = crossEntropy.masked_select(mask).mean()\n",
        "    loss = 0 if math.isnan(loss) else loss\n",
        "    if decoder.use_imemory:\n",
        "        emo_loss = torch.norm(emotion.squeeze(),dim=1)\n",
        "        emo_loss = emo_loss.masked_select(1-mask).mean()\n",
        "        loss += 0 if math.isnan(emo_loss) else emo_loss\n",
        "    loss = loss.to(device)\n",
        "    return loss, nTotal.item()\n",
        "\n",
        "def train(input_variable,input_emotion,lengths,target_variable,mask,max_target_len,encoder,decoder,\n",
        "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
        "\n",
        "    # Zero gradients\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "#     Set device options\n",
        "    input_variable = input_variable.to(device)\n",
        "    lengths = lengths.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    mask = mask.to(device)\n",
        "\n",
        "    # Initialize variables\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "\n",
        "    # Forward pass through encoder\n",
        "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "\n",
        "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
        "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
        "    decoder_input = decoder_input.to(device)\n",
        "    input_emotion = input_emotion.to(device)\n",
        "    \n",
        "\n",
        "    # Set initial decoder hidden state to the encoder's final hidden state\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "    # Determine if we are using teacher forcing this iteration\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    # Forward batch of sequences through decoder one time step at a time\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden,input_emotion = decoder(\n",
        "                decoder_input,input_emotion,decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # Teacher forcing: next input is current target\n",
        "            decoder_input = target_variable[t].view(1, -1)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output,input_emotion,target_variable[t], mask[t], decoder)\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "    else:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden,input_emotion = decoder(\n",
        "                decoder_input,input_emotion,decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # No teacher forcing: next input is decoder's own current output\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "            decoder_input = decoder_input.to(device)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output,input_emotion,target_variable[t], mask[t],decoder)\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "\n",
        "    # Perform backpropatation\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradients: gradients are modified in place\n",
        "    _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "\n",
        "    # Adjust model weights\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return sum(print_losses) / n_totals\n",
        "\n",
        "def trainIters(voc,train_set,train_emo, dev_set,dev_emo, encoder,decoder,encoder_optimizer, \n",
        "               decoder_optimizer,embedding,emo_embedding,encoder_n_layers,decoder_n_layers, \n",
        "               n_iteration,batch_size,print_every,save_every,clip,loadFilename=None):\n",
        "\n",
        "    # Load batches for each iteration\n",
        "#     training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
        "#                       for _ in range(n_iteration)]\n",
        "    training_batches = [voc.batchSent2VecData([random.choice(np.arange(len(train_set))) for _ in range(batch_size)],train_set,train_emo)\n",
        "                      for _ in range(n_iteration)]\n",
        "#     dev_batches = [batch2TrainData(voc, [random.choice(dev_pairs) for _ in range(batch_size)])\n",
        "#                       for _ in range(n_iteration)]\n",
        "    dev_batches = [voc.batchSent2VecData([random.choice(np.arange(len(dev_set))) for _ in range(batch_size)],dev_set,dev_emo)\n",
        "                      for _ in range(n_iteration)]\n",
        "    # Initializations\n",
        "    print('Initializing ...')\n",
        "    start_iteration = 1\n",
        "    print_loss = 0\n",
        "    if loadFilename:\n",
        "        start_iteration = checkpoint['iteration'] + 1\n",
        "\n",
        "    # Training loop\n",
        "    print(\"Training...\")\n",
        "    for iteration in range(start_iteration, n_iteration + 1):\n",
        "        training_batch = training_batches[iteration - 1]\n",
        "        # Extract fields from batch\n",
        "        input_variable, lengths, target_variable, mask, max_target_len,emo_in,emo_out = training_batch\n",
        "        # Run a training iteration with batch\n",
        "        loss = train(input_variable,emo_out,lengths,target_variable,mask, max_target_len,encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
        "\n",
        "\n",
        "        print_loss += loss\n",
        "\n",
        "        # Print progress\n",
        "        if iteration % print_every == 0:\n",
        "            print_loss_avg = print_loss / print_every\n",
        "            print_perplexity = np.exp(print_loss_avg)\n",
        "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}; Average perplexity: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg,print_perplexity))\n",
        "            print_loss = 0\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (iteration % save_every == 0):\n",
        "            dev_batch = dev_batches[iteration-1]\n",
        "            input_variable2, lengths2, target_variable2, mask2, max_target_len2,emo_in2,emo_out2 = dev_batch\n",
        "            dev_loss = train(input_variable2,emo_out2, lengths2, target_variable2, mask2, max_target_len2, encoder,\n",
        "                     decoder,encoder_optimizer, decoder_optimizer,batch_size, clip)\n",
        "            dev_perplexity = np.exp(dev_loss)\n",
        "            \n",
        "            print(\"Iteration: {}; Dev loss: {:.4f}; Dev perplexity: {:.4f}\".format(iteration,dev_loss,dev_perplexity))\n",
        "            directory = os.path.join(PATH, MODEL_NAME)\n",
        "            if not os.path.exists(directory):\n",
        "                os.makedirs(directory)\n",
        "            torch.save({\n",
        "                'iteration': iteration,\n",
        "                'en': encoder.state_dict(),\n",
        "                'de': decoder.state_dict(),\n",
        "                'en_opt': encoder_optimizer.state_dict(),\n",
        "                'de_opt': decoder_optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                'voc_dict': voc.__dict__,\n",
        "                'embedding': embedding.state_dict(),\n",
        "                'emo_embedding':emo_embedding.state_dict()\n",
        "            }, os.path.join(directory,'{}_{}.tar'.format(iteration, 'checkpoint')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3MJbiWsqiRcJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Searcher"
      ]
    },
    {
      "metadata": {
        "id": "dBs2vz9j1Nci",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BeamSearchNode(object):\n",
        "    def __init__(self, hiddenstate,hiddenemotion,previousNode,wordId,logProb,length,past,debug=False):\n",
        "        '''\n",
        "        :param hiddenstate:\n",
        "        :param previousNode:\n",
        "        :param wordId:\n",
        "        :param logProb:\n",
        "        :param length:\n",
        "        '''\n",
        "        self.h = hiddenstate\n",
        "        self.e = hiddenemotion\n",
        "        self.prevNode = previousNode\n",
        "        self.wordid = wordId\n",
        "        self.logp = logProb\n",
        "        self.leng = length\n",
        "        self.past = past\n",
        "        self.debug=debug\n",
        "        if debug:\n",
        "           self.print_node()\n",
        "        \n",
        "    def __lt__(self, other):      \n",
        "#         return self.logp < other.logp\n",
        "        return self.eval() < other.eval()\n",
        "    def eval(self, alpha=1.0):\n",
        "#         if self.leng == 4:\n",
        "#             return 1\n",
        "        reward = 0\n",
        "        # Add here a function for shaping a reward\n",
        "        return self.logp / float(self.leng - 1 + 1e-6) + alpha * reward\n",
        "    def print_node(self):\n",
        "        past_words = [voc.index2word[x] for x in self.past]\n",
        "        print(\"Printing Node {}, loss = {}, eval = {}, len = {},past = {}\".format(voc.index2word[self.wordid.item()],self.logp,self.eval(),self.leng,past_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2wETYjVODDPG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "beam_width = 10\n",
        "max_qsize = 1000\n",
        "min_sentence_length = 7\n",
        "similar_word_len = 2\n",
        "sent_breaker = ['.','.','!','?',':','EOS']\n",
        "class Searcher(nn.Module):\n",
        "    def __init__(self, encoder,decoder,k=1,debug=False):\n",
        "        super(Searcher, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.k = k\n",
        "        self.debug = debug\n",
        "    def forward(self, input_seq,target_emotion,input_length,max_length=MAX_LENGTH):\n",
        "        # Forward input through encoder model\n",
        "        encoder_output, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
        "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
        "        all_scores = torch.zeros([0], device=device)\n",
        "        decoder_emotion = torch.LongTensor([target_emotion]).to(device)\n",
        "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
        "\n",
        "        if self.k == 1:\n",
        "            for _ in range(max_length):\n",
        "                # Forward pass through decoder\n",
        "                decoder_output, decoder_hidden, decoder_emotion = self.decoder(decoder_input,decoder_emotion, decoder_hidden, encoder_output)\n",
        "                # Obtain most likely word token and its softmax score\n",
        "                decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "                # Record token and score\n",
        "                all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "                all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "                # Prepare current token to be next decoder input (add a dimension)\n",
        "                decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "            # Return collections of word tokens and scores\n",
        "            return [all_tokens]\n",
        "        else:\n",
        "            return self.beam_decode(decoder_emotion,decoder_hidden,encoder_output)  \n",
        "    def eval_node(self,decoded_t,n):\n",
        "        length = n.leng\n",
        "        word = voc.index2word[decoded_t.item()]\n",
        "        if word in sent_breaker and length < min_sentence_length:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def beam_decode(self,decoder_emotion,decoder_hidden,encoder_output,debug=False):\n",
        "        endnodes = []\n",
        "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
        "\n",
        "        node = BeamSearchNode(decoder_hidden, decoder_emotion, None, decoder_input, 0, 1,[],debug)\n",
        "        nodes = PriorityQueue()\n",
        "        nodes.put((-node.eval(), node))\n",
        "        qsize = 1\n",
        "        # start beam search\n",
        "        while True:\n",
        "            # give up when decoding takes too long\n",
        "            if qsize > max_qsize or nodes.empty(): break\n",
        "            # fetch the best node\n",
        "            score,n = nodes.get()\n",
        "            decoder_input = n.wordid\n",
        "            decoder_hidden = n.h\n",
        "            if n.wordid.item() == EOS_token and n.prevNode != None:\n",
        "                endnodes.append((score, n))\n",
        "                # if we reached maximum # of sentences required\n",
        "                if len(endnodes) >= topk:\n",
        "                    break\n",
        "            # decode one step using decoder\n",
        "            decoder_output,decoder_hidden,decoder_emotion = self.decoder(decoder_input,decoder_emotion,decoder_hidden,encoder_output)\n",
        "            log_prob, indexes = torch.topk(decoder_output, beam_width)\n",
        "\n",
        "            nextnodes = []\n",
        "#             n.print_node()\n",
        "            for new_k in range(beam_width):\n",
        "                decoded_t = indexes[0][new_k].view(1, -1)    \n",
        "                if not self.eval_node(decoded_t,n):\n",
        "                    continue\n",
        "                log_p = log_prob[0][new_k].item()\n",
        "                past = n.past + [n.wordid.item()]\n",
        "                node = BeamSearchNode(decoder_hidden,decoder_emotion, n,decoded_t,n.logp + log_p, n.leng + 1,past,debug)\n",
        "#                 node.print_node()\n",
        "                score = -node.eval()\n",
        "                nextnodes.append((score, node))\n",
        "            for i in range(len(nextnodes)):\n",
        "                score, nn = nextnodes[i]\n",
        "                nodes.put((score, nn))\n",
        "            qsize += len(nextnodes) - 1\n",
        "\n",
        "        # choose nbest paths, back trace them\n",
        "        if len(endnodes) == 0:\n",
        "            endnodes = [nodes.get() for _ in range(min(100,nodes.qsize()))]\n",
        "        utterances = []\n",
        "        for score, n in sorted(endnodes, key=operator.itemgetter(0)):\n",
        "            if n.leng<min_sentence_length:\n",
        "                continue\n",
        "            utterance = []\n",
        "            utterance.append(n.wordid)\n",
        "            while n.prevNode != None:\n",
        "                n = n.prevNode\n",
        "                utterance.append(n.wordid)\n",
        "\n",
        "            utterance = utterance[::-1]\n",
        "            utterances.append(utterance)\n",
        "\n",
        "        return utterances\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w2KC7N9ydYlV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Searcher test"
      ]
    },
    {
      "metadata": {
        "id": "-DNOoybu2K_Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder.eval()\n",
        "decoder.eval()\n",
        "my_searcher = Searcher(encoder,decoder,2)\n",
        "\n",
        "input_sentence = 'my name is '\n",
        "indexes_batch = [voc.indexesFromSentence(input_sentence,normalize=False)]\n",
        "lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "input_batch = input_batch.to(device)\n",
        "lengths = lengths.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ljDhlIgM4n_T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5e72414e-6562-4528-bf83-ab1aba839176"
      },
      "cell_type": "code",
      "source": [
        "my_searcher = Searcher(encoder,decoder,1)\n",
        "tmp = my_searcher(input_batch,1,lengths)\n",
        "for i in range(min(len(tmp),10)):\n",
        "    decoded_words = [voc.index2word[token.item()] for token in tmp[i]]\n",
        "    decoded_words[:] = [x for x in decoded_words if not (x == 'EOS' or x == 'PAD' or x == 'SOS')]\n",
        "    print('{}: '.format(idx2emo[1]), ' '.join(decoded_words))"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "joy:  i m not a lot . you have to be a lot . you have to be\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q3mcVhd7iRcM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "8adreUoUIWvH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluateInput(searcher,voc,num_output=1,max_length=MAX_LENGTH):\n",
        "    input_sentence = ''\n",
        "    while(1):\n",
        "        try:\n",
        "            input_sentence = input('> ')\n",
        "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
        "            \n",
        "            indexes_batch = [voc.indexesFromSentence(input_sentence,normalize=False)]\n",
        "            lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "            input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
        "            input_batch = input_batch.to(device)\n",
        "            lengths = lengths.to(device)\n",
        "            output = []\n",
        "            for e in range(num_emotion):\n",
        "                tmp = my_searcher(input_batch,e,lengths)\n",
        "                for i in range(min(len(tmp),num_output)):\n",
        "                    decoded_words = [voc.index2word[token.item()] for token in tmp[i]]\n",
        "                    decoded_words[:] = [x for x in decoded_words if not (x == 'EOS' or x == 'PAD' or x == 'SOS')]\n",
        "                    print('{}: '.format(idx2emo[1]), ' '.join(decoded_words))\n",
        "\n",
        "        except KeyError:\n",
        "            print(\"Error: Encountered unknown word.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HEctYPKoiRcO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Run Model"
      ]
    },
    {
      "metadata": {
        "id": "Pajv5qJdmWnb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Configuration"
      ]
    },
    {
      "metadata": {
        "id": "T4Nk1ahmmYRy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Configure models\n",
        "MODEL_NAME = 'use_emb_imem'\n",
        "PATH = 'drive/My Drive/ECM'\n",
        "corpus = 'FullData'\n",
        "\n",
        "use_embedding =  True\n",
        "use_imemory = True\n",
        "use_ememory = False\n",
        "\n",
        "embedding_version = 'self-trained'\n",
        "# embedding_version = 'pre-trained-word2vec-100d'\n",
        "# embedding_version = 'pre-trained-word2vec-300d'\n",
        "# embedding_version = 'pre-trained-subword-100d'\n",
        "\n",
        "attn_model = 'dot'\n",
        "#attn_model = 'general'\n",
        "#attn_model = 'concat'\n",
        "\n",
        "\n",
        "encoder_n_layers = 2\n",
        "decoder_n_layers = 2\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "\n",
        "# Set checkpoint to load from; set to None if starting from scratch\n",
        "loadFilename = \"drive/My Drive/ECM/use_emb_imem/900_checkpoint.tar\"\n",
        "checkpoint_iter = 4000\n",
        "#loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
        "#                            '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
        "#                            '{}_checkpoint.tar'.format(checkpoint_iter))\n",
        "# loadFilename = \"drive/My Drive/ECM/1000_checkpoint.tar\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pU8SMKPJlqce",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ]
    },
    {
      "metadata": {
        "id": "zIO9YhOflsN7",
        "colab_type": "code",
        "outputId": "12e1578c-0c84-42b5-f971-3493153bca5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        }
      },
      "cell_type": "code",
      "source": [
        "datafile = os.path.join(PATH,corpus)\n",
        "voc = Voc(MODEL_NAME, embedding_version)\n",
        "train_set,train_emo,val_set,val_emo,test_set,test_emo = voc.loadDataFromPickle(datafile)\n",
        "\n",
        "# path = 'drive/My Drive/ECM'\n",
        "# corpus = \"CornellMovie\"\n",
        "# filename = \"formatted_movie_lines.txt\"\n",
        "# datafile = os.path.join(path,corpus,filename)\n",
        "# voc = Voc(MODEL_NAME,corpus)\n",
        "# pairs = voc.loadData(datafile)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start preparing data ...\n",
            "loading 10000 th pair from training dataset you get in trouble ? maybe . can t really tell yet .\n",
            "loading 20000 th pair from training dataset is your back still killing you ? i didn t think you d come today .\n",
            "loading 30000 th pair from training dataset ray something s here . where are you pete ?\n",
            "loading 40000 th pair from training dataset thomas could you do me a favor ? what s up ?\n",
            "loading 50000 th pair from training dataset good . i want to discuss with you the mode of payment for the construction of the power plant under well i trust that your presence will hasten the settlement of payment terms .\n",
            "loading 60000 th pair from training dataset well where are you going ? i am going over here .\n",
            "loading 70000 th pair from training dataset that word again ! i don t even know what it means . . . it s time you found out . i love you . i love you . you re a beautiful and\n",
            "loading 80000 th pair from training dataset mr . smith i m wondering whether you have found a way of considering my offer ? i certainly have the will but i wonder if i ve found the way .\n",
            "loading 90000 th pair from training dataset they don t want me up there . every time they look at me i remind them . . . that s not true .\n",
            "loading 110000 th pair from training dataset look here i don t like you getting in my face and saying this bullshit to me . . . that s too bad .\n",
            "loading 120000 th pair from training dataset until we know what choice is there ? we have to stay . but the war the army they need you .\n",
            "loading 130000 th pair from training dataset yeah . . . freddy krueger . freddy that s right . i liked that movie . it was scary .\n",
            "loading 160000 th pair from training dataset how many carats would you like it to be ? i want five carats .\n",
            "loading 180000 th pair from training dataset i m not sure . think carefully ?\n",
            "loading 190000 th pair from training dataset yeah well we re still a little young for albany state prison . are you pissed about al ? i i m getting clear of this . if you re not going to do it for the fucking principle do\n",
            "loading 200000 th pair from training dataset i m afraid not . i ve promised my girlfriend . we re going to travel south to find jobs ok . i d like to say that i ve really enjoyed working with you .however i think you should\n",
            "loading 210000 th pair from training dataset we were due to go back out on the same ship . six months of tests . if you were okay good reason . then what ?\n",
            "loading 220000 th pair from training dataset and what if amateurs try it ? i ll probably kill them . is that likely ?\n",
            "loading 230000 th pair from training dataset what salary do you get for your present position ? rib per month .\n",
            "loading 240000 th pair from training dataset don t even think about it . okay i won t .\n",
            "loading 250000 th pair from training dataset . . .that filth is better left in the sin cities . a .k .a . minneapolis st . paul .\n",
            "loading 260000 th pair from training dataset it s great and everything but what am i going to do with all this ? sell it ?\n",
            "loading 270000 th pair from training dataset what s a pederast walter ? shut the fuck up donny .\n",
            "loading 280000 th pair from training dataset i don t know i just got i got very dizzy . . . i feel dizzy max . well sit down .\n",
            "loading 290000 th pair from training dataset hi tina i ve got good news . i have successfully passed the first two rounds of interview with abc that is awesome . congratulations ! i know you can make it .\n",
            "loading 0 th pair from validation dataset three more i figure . just give me a chip of ice to put in my mouth . just a chip of ice .\n",
            "loading 4000 th pair from validation dataset you don t want to be at crossroads without one . it s a charm that old people teach you i thought you weren t old fashioned .\n",
            "loading 0 th pair from training dataset buddy ? . . . you should be going . . . the primaries are soon aren t they ? new hampshire . . .\n",
            "loading 2000 th pair from training dataset turn on the air conditioner . it doesn t work .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "H7-QZALYluTX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Model"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-09T18:38:37.776215Z",
          "start_time": "2019-04-09T18:38:37.503661Z"
        },
        "id": "-YN94eXeiRcO",
        "colab_type": "code",
        "outputId": "80761996-1c77-4880-e075-e96ed3febdd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "def load_emb(emb_version,loadFilename,hidden_size=500):\n",
        "    embedding = nn.Embedding(voc.num_words, hidden_size)\n",
        "    if emb_version in ['pre-trained-word2vec-100d','pre-trained-word2vec-300d']:\n",
        "            wm = voc.weights_matrix[:voc.num_words]\n",
        "            num_embeddings, embedding_dim = wm.shape\n",
        "            embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
        "            embedding.load_state_dict({'weight': torch.Tensor(wm)})\n",
        "    elif emb_version == 'bpemb':\n",
        "            wm = voc.weights_matrix\n",
        "            num_embeddings, embedding_dim = wm.shape\n",
        "            embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
        "            embedding.load_state_dict({'weight': torch.Tensor(wm)})\n",
        "    if loadFilename:\n",
        "        embedding.load_state_dict(embedding_sd)\n",
        "    return embedding  \n",
        "\n",
        "\n",
        "# Load model if a loadFilename is provided\n",
        "if loadFilename:\n",
        "    # If loading on same machine the model was trained on\n",
        "    checkpoint = torch.load(loadFilename)\n",
        "    # If loading a model trained on GPU to CPU\n",
        "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
        "    encoder_sd = checkpoint['en']\n",
        "    decoder_sd = checkpoint['de']\n",
        "    encoder_optimizer_sd = checkpoint['en_opt']\n",
        "    decoder_optimizer_sd = checkpoint['de_opt']\n",
        "    embedding_sd = checkpoint['embedding']\n",
        "    voc.__dict__ = checkpoint['voc_dict']\n",
        "\n",
        "\n",
        "print('Building encoder and decoder ...')\n",
        "\n",
        "\n",
        "\n",
        "# Initialize word embeddings and emo embedding or load in previous ones\n",
        "embedding = load_emb(embedding_version,loadFilename)\n",
        "emo_embedding = nn.Embedding(num_emotion,embedding.weight.size()[1])\n",
        "\n",
        "\n",
        "# Initialize encoder & decoder models\n",
        "# encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "# decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
        "\n",
        "encoder = EncoderRNN(embedding,encoder_n_layers,dropout)\n",
        "decoder = AttnDecoderRNN(attn_model, embedding, emo_embedding, decoder_n_layers, \n",
        "                         dropout,use_emb=use_embedding, use_imemory=use_imemory, use_ememory=use_ememory)\n",
        "\n",
        "\n",
        "if loadFilename:\n",
        "    encoder.load_state_dict(encoder_sd)\n",
        "    decoder.load_state_dict(decoder_sd)\n",
        "# Use appropriate device\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "print('Models built and ready to go!')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building encoder and decoder ...\n",
            "Models built and ready to go!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hU4mq3qSlwg_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Run Model"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-09T18:44:28.827807Z",
          "start_time": "2019-04-09T18:38:48.666861Z"
        },
        "collapsed": true,
        "id": "YKvesagJiRcR",
        "colab_type": "code",
        "outputId": "036da7ac-0e64-4016-d80d-033d2f55e0c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3821
        }
      },
      "cell_type": "code",
      "source": [
        "# Configure training/optimization\n",
        "clip = 50.0\n",
        "teacher_forcing_ratio = 1.0\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "n_iteration = 1000\n",
        "print_every = 5\n",
        "save_every = 100\n",
        "loadFilename = None\n",
        "\n",
        "# Ensure dropout layers are in train mode\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "# Initialize optimizers\n",
        "print('Building optimizers ...')\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "if loadFilename:\n",
        "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
        "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
        "\n",
        "# Run training iterations\n",
        "print(\"Starting Training!\")\n",
        "# trainIters(voc, train_set[:5000], train_emo[:5000],val_set[:500],val_emo[:500], encoder, decoder, encoder_optimizer, \n",
        "#            decoder_optimizer,embedding, emo_embedding, encoder_n_layers, decoder_n_layers, n_iteration, batch_size,\n",
        "#            print_every, save_every, clip)\n",
        "trainIters(voc, train_set,train_emo,val_set,val_emo, encoder, decoder, encoder_optimizer, \n",
        "           decoder_optimizer,embedding, emo_embedding, encoder_n_layers, decoder_n_layers, n_iteration, batch_size,\n",
        "           print_every, save_every, clip)"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building optimizers ...\n",
            "Starting Training!\n",
            "Initializing ...\n",
            "Training...\n",
            "Iteration: 5; Percent complete: 0.5%; Average loss: 10.5367; Average perplexity: 37674.8678\n",
            "Iteration: 10; Percent complete: 1.0%; Average loss: 8.5017; Average perplexity: 4923.1735\n",
            "Iteration: 15; Percent complete: 1.5%; Average loss: 6.9234; Average perplexity: 1015.8085\n",
            "Iteration: 20; Percent complete: 2.0%; Average loss: 6.2465; Average perplexity: 516.1860\n",
            "Iteration: 25; Percent complete: 2.5%; Average loss: 6.0005; Average perplexity: 403.6250\n",
            "Iteration: 30; Percent complete: 3.0%; Average loss: 6.1390; Average perplexity: 463.5930\n",
            "Iteration: 35; Percent complete: 3.5%; Average loss: 6.0850; Average perplexity: 439.2072\n",
            "Iteration: 40; Percent complete: 4.0%; Average loss: 6.2416; Average perplexity: 513.7022\n",
            "Iteration: 45; Percent complete: 4.5%; Average loss: 6.1131; Average perplexity: 451.7244\n",
            "Iteration: 50; Percent complete: 5.0%; Average loss: 5.9897; Average perplexity: 399.2981\n",
            "Iteration: 55; Percent complete: 5.5%; Average loss: 5.9342; Average perplexity: 377.7370\n",
            "Iteration: 60; Percent complete: 6.0%; Average loss: 6.2259; Average perplexity: 505.6769\n",
            "Iteration: 65; Percent complete: 6.5%; Average loss: 6.0036; Average perplexity: 404.8711\n",
            "Iteration: 70; Percent complete: 7.0%; Average loss: 5.9690; Average perplexity: 391.1115\n",
            "Iteration: 75; Percent complete: 7.5%; Average loss: 6.0215; Average perplexity: 412.1870\n",
            "Iteration: 80; Percent complete: 8.0%; Average loss: 5.8838; Average perplexity: 359.1814\n",
            "Iteration: 85; Percent complete: 8.5%; Average loss: 6.0519; Average perplexity: 424.9168\n",
            "Iteration: 90; Percent complete: 9.0%; Average loss: 5.9714; Average perplexity: 392.0720\n",
            "Iteration: 95; Percent complete: 9.5%; Average loss: 6.0304; Average perplexity: 415.8703\n",
            "Iteration: 100; Percent complete: 10.0%; Average loss: 6.0044; Average perplexity: 405.1923\n",
            "Iteration: 100; Dev loss: 6.0891; Dev perplexity: 441.0327\n",
            "Iteration: 105; Percent complete: 10.5%; Average loss: 6.0170; Average perplexity: 410.3445\n",
            "Iteration: 110; Percent complete: 11.0%; Average loss: 5.8282; Average perplexity: 339.7522\n",
            "Iteration: 115; Percent complete: 11.5%; Average loss: 5.9635; Average perplexity: 388.9814\n",
            "Iteration: 120; Percent complete: 12.0%; Average loss: 5.8964; Average perplexity: 363.7107\n",
            "Iteration: 125; Percent complete: 12.5%; Average loss: 5.9692; Average perplexity: 391.1867\n",
            "Iteration: 130; Percent complete: 13.0%; Average loss: 6.0066; Average perplexity: 406.0865\n",
            "Iteration: 135; Percent complete: 13.5%; Average loss: 5.9572; Average perplexity: 386.5393\n",
            "Iteration: 140; Percent complete: 14.0%; Average loss: 5.9141; Average perplexity: 370.2255\n",
            "Iteration: 145; Percent complete: 14.5%; Average loss: 5.9594; Average perplexity: 387.3749\n",
            "Iteration: 150; Percent complete: 15.0%; Average loss: 5.9700; Average perplexity: 391.5233\n",
            "Iteration: 155; Percent complete: 15.5%; Average loss: 5.9480; Average perplexity: 382.9974\n",
            "Iteration: 160; Percent complete: 16.0%; Average loss: 5.8528; Average perplexity: 348.2170\n",
            "Iteration: 165; Percent complete: 16.5%; Average loss: 5.9464; Average perplexity: 382.3606\n",
            "Iteration: 170; Percent complete: 17.0%; Average loss: 5.9343; Average perplexity: 377.7757\n",
            "Iteration: 175; Percent complete: 17.5%; Average loss: 5.8982; Average perplexity: 364.3861\n",
            "Iteration: 180; Percent complete: 18.0%; Average loss: 5.9081; Average perplexity: 367.9932\n",
            "Iteration: 185; Percent complete: 18.5%; Average loss: 5.8893; Average perplexity: 361.1397\n",
            "Iteration: 190; Percent complete: 19.0%; Average loss: 5.9310; Average perplexity: 376.5386\n",
            "Iteration: 195; Percent complete: 19.5%; Average loss: 5.9533; Average perplexity: 385.0325\n",
            "Iteration: 200; Percent complete: 20.0%; Average loss: 5.8439; Average perplexity: 345.1150\n",
            "Iteration: 200; Dev loss: 5.8082; Dev perplexity: 333.0187\n",
            "Iteration: 205; Percent complete: 20.5%; Average loss: 5.8337; Average perplexity: 341.6328\n",
            "Iteration: 210; Percent complete: 21.0%; Average loss: 5.8919; Average perplexity: 362.0837\n",
            "Iteration: 215; Percent complete: 21.5%; Average loss: 5.8263; Average perplexity: 339.1118\n",
            "Iteration: 220; Percent complete: 22.0%; Average loss: 5.8951; Average perplexity: 363.2434\n",
            "Iteration: 225; Percent complete: 22.5%; Average loss: 5.8478; Average perplexity: 346.4678\n",
            "Iteration: 230; Percent complete: 23.0%; Average loss: 5.8244; Average perplexity: 338.4662\n",
            "Iteration: 235; Percent complete: 23.5%; Average loss: 5.7941; Average perplexity: 328.3498\n",
            "Iteration: 240; Percent complete: 24.0%; Average loss: 5.9614; Average perplexity: 388.1579\n",
            "Iteration: 245; Percent complete: 24.5%; Average loss: 5.9055; Average perplexity: 367.0343\n",
            "Iteration: 250; Percent complete: 25.0%; Average loss: 5.7801; Average perplexity: 323.7823\n",
            "Iteration: 255; Percent complete: 25.5%; Average loss: 5.7680; Average perplexity: 319.8949\n",
            "Iteration: 260; Percent complete: 26.0%; Average loss: 5.7302; Average perplexity: 308.0412\n",
            "Iteration: 265; Percent complete: 26.5%; Average loss: 5.7914; Average perplexity: 327.4873\n",
            "Iteration: 270; Percent complete: 27.0%; Average loss: 5.8453; Average perplexity: 345.6052\n",
            "Iteration: 275; Percent complete: 27.5%; Average loss: 5.8210; Average perplexity: 337.3252\n",
            "Iteration: 280; Percent complete: 28.0%; Average loss: 5.7075; Average perplexity: 301.1196\n",
            "Iteration: 285; Percent complete: 28.5%; Average loss: 5.7547; Average perplexity: 315.6785\n",
            "Iteration: 290; Percent complete: 29.0%; Average loss: 5.7825; Average perplexity: 324.5830\n",
            "Iteration: 295; Percent complete: 29.5%; Average loss: 5.7395; Average perplexity: 310.9219\n",
            "Iteration: 300; Percent complete: 30.0%; Average loss: 5.7465; Average perplexity: 313.0804\n",
            "Iteration: 300; Dev loss: 5.6712; Dev perplexity: 290.3767\n",
            "Iteration: 305; Percent complete: 30.5%; Average loss: 5.7079; Average perplexity: 301.2474\n",
            "Iteration: 310; Percent complete: 31.0%; Average loss: 5.7764; Average perplexity: 322.5833\n",
            "Iteration: 315; Percent complete: 31.5%; Average loss: 5.7537; Average perplexity: 315.3588\n",
            "Iteration: 320; Percent complete: 32.0%; Average loss: 5.8178; Average perplexity: 336.2419\n",
            "Iteration: 325; Percent complete: 32.5%; Average loss: 5.9303; Average perplexity: 376.2687\n",
            "Iteration: 330; Percent complete: 33.0%; Average loss: 5.7533; Average perplexity: 315.2281\n",
            "Iteration: 335; Percent complete: 33.5%; Average loss: 5.7105; Average perplexity: 302.0229\n",
            "Iteration: 340; Percent complete: 34.0%; Average loss: 5.5219; Average perplexity: 250.1166\n",
            "Iteration: 345; Percent complete: 34.5%; Average loss: 5.7559; Average perplexity: 316.0401\n",
            "Iteration: 350; Percent complete: 35.0%; Average loss: 5.8151; Average perplexity: 335.3375\n",
            "Iteration: 355; Percent complete: 35.5%; Average loss: 5.6945; Average perplexity: 297.2147\n",
            "Iteration: 360; Percent complete: 36.0%; Average loss: 5.7335; Average perplexity: 309.0621\n",
            "Iteration: 365; Percent complete: 36.5%; Average loss: 5.6880; Average perplexity: 295.2969\n",
            "Iteration: 370; Percent complete: 37.0%; Average loss: 5.6105; Average perplexity: 273.2712\n",
            "Iteration: 375; Percent complete: 37.5%; Average loss: 5.5875; Average perplexity: 267.0561\n",
            "Iteration: 380; Percent complete: 38.0%; Average loss: 5.6828; Average perplexity: 293.7771\n",
            "Iteration: 385; Percent complete: 38.5%; Average loss: 5.6896; Average perplexity: 295.7762\n",
            "Iteration: 390; Percent complete: 39.0%; Average loss: 5.6763; Average perplexity: 291.8665\n",
            "Iteration: 395; Percent complete: 39.5%; Average loss: 5.6507; Average perplexity: 284.4858\n",
            "Iteration: 400; Percent complete: 40.0%; Average loss: 5.6615; Average perplexity: 287.5786\n",
            "Iteration: 400; Dev loss: 5.6552; Dev perplexity: 285.7740\n",
            "Iteration: 405; Percent complete: 40.5%; Average loss: 5.5645; Average perplexity: 260.9834\n",
            "Iteration: 410; Percent complete: 41.0%; Average loss: 5.6609; Average perplexity: 287.4144\n",
            "Iteration: 415; Percent complete: 41.5%; Average loss: 5.6583; Average perplexity: 286.6482\n",
            "Iteration: 420; Percent complete: 42.0%; Average loss: 5.6084; Average perplexity: 272.7129\n",
            "Iteration: 425; Percent complete: 42.5%; Average loss: 5.6022; Average perplexity: 271.0179\n",
            "Iteration: 430; Percent complete: 43.0%; Average loss: 5.6415; Average perplexity: 281.8936\n",
            "Iteration: 435; Percent complete: 43.5%; Average loss: 5.7708; Average perplexity: 320.7874\n",
            "Iteration: 440; Percent complete: 44.0%; Average loss: 5.6115; Average perplexity: 273.5492\n",
            "Iteration: 445; Percent complete: 44.5%; Average loss: 5.5454; Average perplexity: 256.0556\n",
            "Iteration: 450; Percent complete: 45.0%; Average loss: 5.5046; Average perplexity: 245.8296\n",
            "Iteration: 455; Percent complete: 45.5%; Average loss: 5.5143; Average perplexity: 248.2193\n",
            "Iteration: 460; Percent complete: 46.0%; Average loss: 5.5907; Average perplexity: 267.9108\n",
            "Iteration: 465; Percent complete: 46.5%; Average loss: 5.6001; Average perplexity: 270.4495\n",
            "Iteration: 470; Percent complete: 47.0%; Average loss: 5.5836; Average perplexity: 266.0290\n",
            "Iteration: 475; Percent complete: 47.5%; Average loss: 5.5535; Average perplexity: 258.1284\n",
            "Iteration: 480; Percent complete: 48.0%; Average loss: 5.5140; Average perplexity: 248.1507\n",
            "Iteration: 485; Percent complete: 48.5%; Average loss: 5.4946; Average perplexity: 243.3780\n",
            "Iteration: 490; Percent complete: 49.0%; Average loss: 5.5904; Average perplexity: 267.8546\n",
            "Iteration: 495; Percent complete: 49.5%; Average loss: 5.4864; Average perplexity: 241.3893\n",
            "Iteration: 500; Percent complete: 50.0%; Average loss: 5.4787; Average perplexity: 239.5263\n",
            "Iteration: 500; Dev loss: 5.4482; Dev perplexity: 232.3512\n",
            "Iteration: 505; Percent complete: 50.5%; Average loss: 5.4534; Average perplexity: 233.5479\n",
            "Iteration: 510; Percent complete: 51.0%; Average loss: 5.4493; Average perplexity: 232.6051\n",
            "Iteration: 515; Percent complete: 51.5%; Average loss: 5.3220; Average perplexity: 204.7848\n",
            "Iteration: 520; Percent complete: 52.0%; Average loss: 5.4419; Average perplexity: 230.8721\n",
            "Iteration: 525; Percent complete: 52.5%; Average loss: 5.5018; Average perplexity: 245.1390\n",
            "Iteration: 530; Percent complete: 53.0%; Average loss: 5.5069; Average perplexity: 246.3754\n",
            "Iteration: 535; Percent complete: 53.5%; Average loss: 5.3862; Average perplexity: 218.3786\n",
            "Iteration: 540; Percent complete: 54.0%; Average loss: 5.4018; Average perplexity: 221.8049\n",
            "Iteration: 545; Percent complete: 54.5%; Average loss: 5.4672; Average perplexity: 236.7864\n",
            "Iteration: 550; Percent complete: 55.0%; Average loss: 5.3533; Average perplexity: 211.3036\n",
            "Iteration: 555; Percent complete: 55.5%; Average loss: 5.4423; Average perplexity: 230.9710\n",
            "Iteration: 560; Percent complete: 56.0%; Average loss: 5.3309; Average perplexity: 206.6174\n",
            "Iteration: 565; Percent complete: 56.5%; Average loss: 5.3892; Average perplexity: 219.0195\n",
            "Iteration: 570; Percent complete: 57.0%; Average loss: 5.3847; Average perplexity: 218.0420\n",
            "Iteration: 575; Percent complete: 57.5%; Average loss: 5.4039; Average perplexity: 222.2812\n",
            "Iteration: 580; Percent complete: 58.0%; Average loss: 5.2854; Average perplexity: 197.4317\n",
            "Iteration: 585; Percent complete: 58.5%; Average loss: 5.2669; Average perplexity: 193.8171\n",
            "Iteration: 590; Percent complete: 59.0%; Average loss: 5.2430; Average perplexity: 189.2358\n",
            "Iteration: 595; Percent complete: 59.5%; Average loss: 5.1791; Average perplexity: 177.5202\n",
            "Iteration: 600; Percent complete: 60.0%; Average loss: 5.3663; Average perplexity: 214.0730\n",
            "Iteration: 600; Dev loss: 5.0835; Dev perplexity: 161.3457\n",
            "Iteration: 605; Percent complete: 60.5%; Average loss: 5.3690; Average perplexity: 214.6423\n",
            "Iteration: 610; Percent complete: 61.0%; Average loss: 5.3216; Average perplexity: 204.7151\n",
            "Iteration: 615; Percent complete: 61.5%; Average loss: 5.1811; Average perplexity: 177.8769\n",
            "Iteration: 620; Percent complete: 62.0%; Average loss: 5.2842; Average perplexity: 197.1983\n",
            "Iteration: 625; Percent complete: 62.5%; Average loss: 5.2599; Average perplexity: 192.4547\n",
            "Iteration: 630; Percent complete: 63.0%; Average loss: 5.1160; Average perplexity: 166.6624\n",
            "Iteration: 635; Percent complete: 63.5%; Average loss: 5.1715; Average perplexity: 176.1709\n",
            "Iteration: 640; Percent complete: 64.0%; Average loss: 5.1420; Average perplexity: 171.0540\n",
            "Iteration: 645; Percent complete: 64.5%; Average loss: 5.1690; Average perplexity: 175.7397\n",
            "Iteration: 650; Percent complete: 65.0%; Average loss: 5.2168; Average perplexity: 184.3504\n",
            "Iteration: 655; Percent complete: 65.5%; Average loss: 5.1871; Average perplexity: 178.9429\n",
            "Iteration: 660; Percent complete: 66.0%; Average loss: 5.1255; Average perplexity: 168.2518\n",
            "Iteration: 665; Percent complete: 66.5%; Average loss: 5.2939; Average perplexity: 199.1259\n",
            "Iteration: 670; Percent complete: 67.0%; Average loss: 5.1476; Average perplexity: 172.0141\n",
            "Iteration: 675; Percent complete: 67.5%; Average loss: 5.0819; Average perplexity: 161.0752\n",
            "Iteration: 680; Percent complete: 68.0%; Average loss: 5.2659; Average perplexity: 193.6112\n",
            "Iteration: 685; Percent complete: 68.5%; Average loss: 5.0085; Average perplexity: 149.6782\n",
            "Iteration: 690; Percent complete: 69.0%; Average loss: 5.1543; Average perplexity: 173.1763\n",
            "Iteration: 695; Percent complete: 69.5%; Average loss: 5.1214; Average perplexity: 167.5679\n",
            "Iteration: 700; Percent complete: 70.0%; Average loss: 5.0141; Average perplexity: 150.5137\n",
            "Iteration: 700; Dev loss: 4.9073; Dev perplexity: 135.2683\n",
            "Iteration: 705; Percent complete: 70.5%; Average loss: 5.1773; Average perplexity: 177.2033\n",
            "Iteration: 710; Percent complete: 71.0%; Average loss: 5.1179; Average perplexity: 166.9844\n",
            "Iteration: 715; Percent complete: 71.5%; Average loss: 5.1113; Average perplexity: 165.8923\n",
            "Iteration: 720; Percent complete: 72.0%; Average loss: 4.9962; Average perplexity: 147.8464\n",
            "Iteration: 725; Percent complete: 72.5%; Average loss: 5.0467; Average perplexity: 155.5044\n",
            "Iteration: 730; Percent complete: 73.0%; Average loss: 5.0362; Average perplexity: 153.8875\n",
            "Iteration: 735; Percent complete: 73.5%; Average loss: 5.1215; Average perplexity: 167.5851\n",
            "Iteration: 740; Percent complete: 74.0%; Average loss: 5.1392; Average perplexity: 170.5751\n",
            "Iteration: 745; Percent complete: 74.5%; Average loss: 5.1327; Average perplexity: 169.4807\n",
            "Iteration: 750; Percent complete: 75.0%; Average loss: 5.0260; Average perplexity: 152.3262\n",
            "Iteration: 755; Percent complete: 75.5%; Average loss: 5.0592; Average perplexity: 157.4574\n",
            "Iteration: 760; Percent complete: 76.0%; Average loss: 5.1295; Average perplexity: 168.9392\n",
            "Iteration: 765; Percent complete: 76.5%; Average loss: 5.0047; Average perplexity: 149.1099\n",
            "Iteration: 770; Percent complete: 77.0%; Average loss: 5.0836; Average perplexity: 161.3480\n",
            "Iteration: 775; Percent complete: 77.5%; Average loss: 4.9919; Average perplexity: 147.2207\n",
            "Iteration: 780; Percent complete: 78.0%; Average loss: 4.8909; Average perplexity: 133.0788\n",
            "Iteration: 785; Percent complete: 78.5%; Average loss: 4.9022; Average perplexity: 134.5852\n",
            "Iteration: 790; Percent complete: 79.0%; Average loss: 4.9836; Average perplexity: 145.9945\n",
            "Iteration: 795; Percent complete: 79.5%; Average loss: 5.0301; Average perplexity: 152.9547\n",
            "Iteration: 800; Percent complete: 80.0%; Average loss: 4.8631; Average perplexity: 129.4291\n",
            "Iteration: 800; Dev loss: 4.9862; Dev perplexity: 146.3744\n",
            "Iteration: 805; Percent complete: 80.5%; Average loss: 4.9598; Average perplexity: 142.5693\n",
            "Iteration: 810; Percent complete: 81.0%; Average loss: 4.9682; Average perplexity: 143.7677\n",
            "Iteration: 815; Percent complete: 81.5%; Average loss: 5.0425; Average perplexity: 154.8509\n",
            "Iteration: 820; Percent complete: 82.0%; Average loss: 4.9773; Average perplexity: 145.0872\n",
            "Iteration: 825; Percent complete: 82.5%; Average loss: 5.0233; Average perplexity: 151.9105\n",
            "Iteration: 830; Percent complete: 83.0%; Average loss: 4.9675; Average perplexity: 143.6723\n",
            "Iteration: 835; Percent complete: 83.5%; Average loss: 4.9583; Average perplexity: 142.3468\n",
            "Iteration: 840; Percent complete: 84.0%; Average loss: 4.8475; Average perplexity: 127.4235\n",
            "Iteration: 845; Percent complete: 84.5%; Average loss: 4.9976; Average perplexity: 148.0623\n",
            "Iteration: 850; Percent complete: 85.0%; Average loss: 4.9395; Average perplexity: 139.6972\n",
            "Iteration: 855; Percent complete: 85.5%; Average loss: 4.8568; Average perplexity: 128.6144\n",
            "Iteration: 860; Percent complete: 86.0%; Average loss: 4.9067; Average perplexity: 135.1915\n",
            "Iteration: 865; Percent complete: 86.5%; Average loss: 4.8492; Average perplexity: 127.6402\n",
            "Iteration: 870; Percent complete: 87.0%; Average loss: 4.8591; Average perplexity: 128.9059\n",
            "Iteration: 875; Percent complete: 87.5%; Average loss: 4.8938; Average perplexity: 133.4583\n",
            "Iteration: 880; Percent complete: 88.0%; Average loss: 4.8855; Average perplexity: 132.3562\n",
            "Iteration: 885; Percent complete: 88.5%; Average loss: 4.8785; Average perplexity: 131.4300\n",
            "Iteration: 890; Percent complete: 89.0%; Average loss: 4.8700; Average perplexity: 130.3164\n",
            "Iteration: 895; Percent complete: 89.5%; Average loss: 4.8370; Average perplexity: 126.0882\n",
            "Iteration: 900; Percent complete: 90.0%; Average loss: 5.0491; Average perplexity: 155.8803\n",
            "Iteration: 900; Dev loss: 4.6250; Dev perplexity: 102.0038\n",
            "Iteration: 905; Percent complete: 90.5%; Average loss: 4.9460; Average perplexity: 140.6087\n",
            "Iteration: 910; Percent complete: 91.0%; Average loss: 4.8546; Average perplexity: 128.3276\n",
            "Iteration: 915; Percent complete: 91.5%; Average loss: 4.9004; Average perplexity: 134.3414\n",
            "Iteration: 920; Percent complete: 92.0%; Average loss: 4.8391; Average perplexity: 126.3498\n",
            "Iteration: 925; Percent complete: 92.5%; Average loss: 4.8654; Average perplexity: 129.7255\n",
            "Iteration: 930; Percent complete: 93.0%; Average loss: 4.8512; Average perplexity: 127.8932\n",
            "Iteration: 935; Percent complete: 93.5%; Average loss: 4.9652; Average perplexity: 143.3327\n",
            "Iteration: 940; Percent complete: 94.0%; Average loss: 4.9584; Average perplexity: 142.3626\n",
            "Iteration: 945; Percent complete: 94.5%; Average loss: 4.9850; Average perplexity: 146.2081\n",
            "Iteration: 950; Percent complete: 95.0%; Average loss: 4.8481; Average perplexity: 127.4934\n",
            "Iteration: 955; Percent complete: 95.5%; Average loss: 4.9398; Average perplexity: 139.7382\n",
            "Iteration: 960; Percent complete: 96.0%; Average loss: 4.8671; Average perplexity: 129.9444\n",
            "Iteration: 965; Percent complete: 96.5%; Average loss: 4.8088; Average perplexity: 122.5854\n",
            "Iteration: 970; Percent complete: 97.0%; Average loss: 4.8737; Average perplexity: 130.8105\n",
            "Iteration: 975; Percent complete: 97.5%; Average loss: 4.8855; Average perplexity: 132.3555\n",
            "Iteration: 980; Percent complete: 98.0%; Average loss: 4.8442; Average perplexity: 127.0023\n",
            "Iteration: 985; Percent complete: 98.5%; Average loss: 4.8496; Average perplexity: 127.6890\n",
            "Iteration: 990; Percent complete: 99.0%; Average loss: 4.7883; Average perplexity: 120.1003\n",
            "Iteration: 995; Percent complete: 99.5%; Average loss: 4.7312; Average perplexity: 113.4320\n",
            "Iteration: 1000; Percent complete: 100.0%; Average loss: 4.8919; Average perplexity: 133.2069\n",
            "Iteration: 1000; Dev loss: 4.7530; Dev perplexity: 115.9346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CcdXbJBPypOs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluate"
      ]
    },
    {
      "metadata": {
        "id": "qYlQ6dtt2z-3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-09T18:45:09.842889Z",
          "start_time": "2019-04-09T18:44:54.752137Z"
        },
        "id": "IpBd_dC1iRcU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1362
        },
        "outputId": "d1cb2414-b260-4c3c-e9da-6677e1c2d072"
      },
      "cell_type": "code",
      "source": [
        "# Set dropout layers to eval mode\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "# Initialize search module\n",
        "# searcher = GreedySearchDecoder(encoder, decoder)\n",
        "searcher = MyTopKDecoder(encoder, decoder,3)\n",
        "\n",
        "# Begin chatting (uncomment and run the following line to begin)\n",
        "evaluateInput(encoder, decoder, searcher,voc,num_output=3)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> hello\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-201eebdaf7b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Begin chatting (uncomment and run the following line to begin)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mevaluateInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvoc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-940626b4b47b>\u001b[0m in \u001b[0;36mevaluateInput\u001b[0;34m(encoder, decoder, searcher, voc, num_output, max_length)\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearcher\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mMyTopKDecoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-bd72ba5dc85a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_seq, target_emotion, input_length, num_output, max_length)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mall_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0memotion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_emotion\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbeam_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mGreedySearchDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-05b2a1c1ae02>\u001b[0m in \u001b[0;36mbeam_decode\u001b[0;34m(decoder, decoder_emotion, decoder_hidden, encoder_output, topk, num_output, debug)\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# decode for one step using decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_emotion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_emotion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;31m#         decoder_hidden = decoder_hidden[:, idx, :].unsqueeze(0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m#         decoder_hidden = torch.unsqueeze(decoder_hidden,0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-d3faab8b0fbe>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_step, emotion, last_hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mconcat_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;31m# Predict next word using Luong eq. 6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memotion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Gg6M75fPLjfJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Save Model"
      ]
    },
    {
      "metadata": {
        "id": "GYVH2cC1L24x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# !zip -r cb_model.zip cb_model\n",
        "# !cp cb_model.zip 'gdrive/My Drive/'\n",
        "\n",
        "# content/ecm_model_pretrained2/DailyDialogue/2-2_300/1400_checkpoint.tar\n",
        "# content/ecm_model_pretrained2/DailyDialogue/2-2_300/2000_checkpoint.tar\n",
        "# content/ecm_model_withimemory_embfix/DailyDialogue/2-2_300/2000_checkpoint.tar\n",
        "# content/ecm_model_withimemory/DailyDialogue/2-2_100/2000_checkpoint.tar\n",
        "# ! cp ecm_model_withimemory/DailyDialogue/2-2_300/2000_checkpoint.tar \n",
        "# content/ecm_model_withimemory_bpemb/DailyDialogue/2-2_100/5000_checkpoint.tar\n",
        "# content/ecm_model_imemory_bpemb_notfix/DailyDialogue/2-2_100/5000_checkpoint.tar\n",
        "# content/ecm_model_imemory_bpemb_fix/CornellMovie/2-2_100/10000_checkpoint.tar\n",
        "# content/1000_checkpoint.tar\n",
        "# ! mv ecm_model_imemory_bpemb_fix/CornellMovie/2-2_100/10000_checkpoint.tar ecm_imemory_bpemb_nofix_10000_cornell_checkpoint.tar\n",
        "! cp 1000_checkpoint.tar 'drive/My Drive/ECM'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oXpk7gFoowOs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Debug"
      ]
    },
    {
      "metadata": {
        "id": "oYfuA4oZKX3a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filepath = 'drive/My Drive/ECM/FullData'\n",
        "f = open(filepath+'/train.pickle', 'rb')\n",
        "train_data,train_emo_set = pickle.load(f),pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6LHpBikRKn-X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2993b4f5-83e5-4522-8df2-029287b933cb"
      },
      "cell_type": "code",
      "source": [
        "torch.FloatTensor(10)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2.1851e+17, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "metadata": {
        "id": "fZGsRbH5Sdkk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2167045b-d861-4937-8bc4-350aa4c23f69"
      },
      "cell_type": "code",
      "source": [
        "torch.randn(10,dtype=torch.float)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.9379, -1.5944,  0.0552,  1.4294, -0.1484,  0.9212, -0.6980,  0.8830,\n",
              "         0.5407,  2.8342])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "metadata": {
        "id": "eyzlMiuC6BEN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## TrainIter"
      ]
    },
    {
      "metadata": {
        "id": "mo115MEoHS7W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "W6vsitmj6AdO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batches = voc.batchSent2VecData([random.choice(np.arange(len(val_set))) for _ in range(64)],val_set,val_emo)\n",
        "input_variable, lengths, target_variable, mask, max_target_len,emo_in,emo_out = batches\n",
        "# loss = train(input_variable, emo_out, lengths, target_variable, mask, max_target_len, encoder,\n",
        "#                      decoder, encoder_optimizer, decoder_optimizer, batch_size, clip)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0MJ_JNy96PjG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train"
      ]
    },
    {
      "metadata": {
        "id": "PGQcuGIF6Rml",
        "colab_type": "code",
        "outputId": "782e12e5-7d5a-4dea-b2a1-47bd2fc57380",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "input_variable = input_variable.to(device)\n",
        "lengths = lengths.to(device)\n",
        "target_variable = target_variable.to(device)\n",
        "mask = mask.to(device)\n",
        "\n",
        "\n",
        "encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "\n",
        "\n",
        "decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
        "decoder_input = decoder_input.to(device)\n",
        "input_emotion = emo_out.to(device)\n",
        "decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "\n",
        "decoder_output, decoder_hidden,input_emotion = decoder(decoder_input,input_emotion,decoder_hidden, encoder_outputs)\n",
        "\n",
        "\n",
        "_, topi = decoder_output.topk(1)\n",
        "decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "decoder_input = decoder_input.to(device)\n",
        "loss = 0\n",
        "for t in range(max_target_len):\n",
        "    mask_loss,nTotal = maskNLLLoss(decoder_output,input_emotion,target_variable[t],mask[t],decoder)\n",
        "    loss += mask_loss\n",
        "loss.backward()"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encounter error loss is NAN\n",
            "encounter error loss is NAN\n",
            "encounter error loss is NAN\n",
            "encounter error loss is NAN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ajmYNiouRpU-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "dda560ae-b628-4fec-ef78-cb8be8069d6c"
      },
      "cell_type": "code",
      "source": [
        "t = 0\n",
        "maskNLLLoss(decoder_output,input_emotion,target_variable[t],mask[t],decoder)\n"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encounter error loss is NAN\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(10.7799, grad_fn=<MeanBackward1>), 64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "metadata": {
        "id": "Q1xfa92iIFLQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "0fd6285f-2d63-4e79-f94e-f62d1c69d385"
      },
      "cell_type": "code",
      "source": [
        "# print(encoder_outputs, encoder_hidden)\n",
        "# emo_embedding.weight\n",
        "embedding.weight"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
              "        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
              "        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
              "        ...,\n",
              "        [ 1.2485, -0.8687, -0.4576,  ..., -0.6730,  1.0463,  2.1569],\n",
              "        [-0.2561,  0.4457, -0.3998,  ..., -1.6912,  0.3333,  0.2043],\n",
              "        [ 1.1599,  0.6201, -0.2096,  ..., -0.9545, -0.9864, -0.2484]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "metadata": {
        "id": "apXB_YgYdTnl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70ac80c0-cd2b-471c-ff87-a575802e5a7b"
      },
      "cell_type": "code",
      "source": [
        "print(decoder_output.size(),decoder_hidden.size(),input_emotion.size())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 47651]) torch.Size([2, 64, 500]) torch.Size([1, 64, 500])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zNZPLStw6hPi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Encoder"
      ]
    },
    {
      "metadata": {
        "id": "PA_YBHUg-0aW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "970534ac-7190-438b-f0a9-c57ecd1c00a4"
      },
      "cell_type": "code",
      "source": [
        "# embedding(input_variable).size()\n",
        "emo_embedding(emo_out).unsqueeze(dim=0).size()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b73399546f88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memo_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memo_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'emo_embedding' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Yjii515SpFZ_",
        "colab_type": "code",
        "outputId": "823a6f34-08d5-47db-b0cb-906fc26782f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        }
      },
      "cell_type": "code",
      "source": [
        "print(input_variable.shape)\n",
        "embedded = embedding(input_variable)\n",
        "#         # Pack padded batch of sequences for RNN module\n",
        "#         packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "#         # Forward pass through GRU\n",
        "#         outputs, hidden = self.gru(packed, hidden)\n",
        "#         # Unpack padding\n",
        "#         outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "#         # Sum bidirectional GRU outputs\n",
        "#         outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([65, 64])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-f954b0768386>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_variable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_variable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#         # Pack padded batch of sequences for RNN module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#         packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#         # Forward pass through GRU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m         return F.embedding(\n\u001b[1;32m    117\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1452\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1454\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: index out of range at /pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp:191"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "zKGUh6lTpZyQ",
        "colab_type": "code",
        "outputId": "d9eba29e-85a8-4dd4-fa68-030bc23e2457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# embedding2 = nn.Embedding(voc.num_words, 500)\n",
        "# embedding2(input_variable)\n",
        "# print(embedding,input_variable.size())\n",
        "# voc.num_words\n",
        "print(torch.max(input_variable))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(47479)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8Xqi4nvc6jlr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, embedding, n_layers=1, dropout=0):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = embedding.weight.size()[1]\n",
        "        self.embedding = embedding\n",
        "\n",
        "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
        "        #   because our input size is a word embedding with number of features == hidden_size\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, n_layers,\n",
        "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
        "\n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "        # Convert word indexes to embeddings\n",
        "        embedded = self.embedding(input_seq)\n",
        "        # Pack padded batch of sequences for RNN module\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "        # Forward pass through GRU\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "        # Unpack padding\n",
        "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        # Sum bidirectional GRU outputs\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
        "        # Return output and final hidden state\n",
        "        return outputs, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f4BuAXU36tR2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Decoder"
      ]
    },
    {
      "metadata": {
        "id": "uixwt724GXBD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "c9af7cca-4d7e-4074-edca-ad92958bc506"
      },
      "cell_type": "code",
      "source": [
        "batches = voc.batchSent2VecData([random.choice(np.arange(len(pairs))) for _ in range(64)],pairs)\n",
        "input_variable, lengths, target_variable, mask, max_target_len,emo_in,emo_out = batches\n",
        "\n",
        "# Define path to new file\n",
        "# datafile = \"drive/My Drive/ECM/CornellMovie/formatted_movie_lines.txt\"\n",
        "# voc2, pairs2 = loadPrepareData(datafile)\n",
        "batches2 = batch2TrainData(voc2, [random.choice(pairs2) for _ in range(64)])\n",
        "input_variable2, lengths2, target_variable2, mask2, max_target_len2 = batches2\n",
        "\n",
        "# print(\"input_variable:\", input_variable)\n",
        "# print(\"lengths:\", lengths2)\n",
        "# print(\"target_variable:\", target_variable)\n",
        "# print(\"mask:\", mask)\n",
        "# print(\"max_target_len:\", max_target_len)\n",
        "# input_variable2 = input_variable2.to(device)\n",
        "# lengths2 = lengths2.to(device)\n",
        "# hidden=None\n",
        "# embedded2 = embedding(input_variable2)\n",
        "# torch.as_tensor(lengths2, dtype=torch.int64)\n",
        "# packed2 = torch.nn.utils.rnn.pack_padded_sequence(embedded2, lengths2)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-0d6aef1555f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchSent2VecData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0minput_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_target_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memo_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memo_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define path to new file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# datafile = \"drive/My Drive/ECM/CornellMovie/formatted_movie_lines.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-0d6aef1555f8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchSent2VecData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0minput_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_target_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memo_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memo_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define path to new file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# datafile = \"drive/My Drive/ECM/CornellMovie/formatted_movie_lines.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pairs' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "uSjVnv1FGo2h",
        "colab_type": "code",
        "outputId": "a4e0142c-ec3b-4305-8fb3-59d47e60f15d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "cell_type": "code",
      "source": [
        "print(lengths2)\n",
        "print(lengths)\n",
        "# packed2 = torch.nn.utils.rnn.pack_padded_sequence(embedded, lengths)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([20, 20, 19, 19, 17, 17, 17, 17, 17, 16, 16, 15, 13, 13, 13, 12, 12, 11,\n",
            "        11, 11, 11, 11, 10, 10, 10, 10,  9,  9,  9,  9,  9,  8,  8,  8,  8,  8,\n",
            "         8,  8,  8,  7,  7,  7,  7,  6,  6,  6,  6,  6,  6,  5,  5,  5,  5,  5,\n",
            "         4,  4,  4,  4,  4,  3,  3,  3,  3,  3])\n",
            "tensor([22, 22, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 20, 20, 20, 18,\n",
            "        18, 17, 16, 14, 14, 14, 13, 13, 13, 12, 12, 12, 12, 11, 11, 10, 10, 10,\n",
            "        10,  9,  9,  9,  9,  9,  9,  9,  8,  8,  8,  7,  7,  7,  7,  7,  6,  6,\n",
            "         6,  6,  5,  5,  5,  5,  4,  4,  3,  3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sS1pBHog5h64",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
        "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
        "    decoder_input = decoder_input.to(device)\n",
        "\n",
        "    # Set initial decoder hidden state to the encoder's final hidden state\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "# No teacher forcing: next input is decoder's own current output\n",
        "_, topi = decoder_output.topk(1)\n",
        "decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "decoder_input = decoder_input.to(device)\n",
        "# Calculate and accumulate loss\n",
        "mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "loss += mask_loss\n",
        "print_losses.append(mask_loss.item() * nTotal)\n",
        "n_totals += nTotal"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XCNsW_hoFslB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
        "input_variable, lengths, target_variable, mask, max_target_len = batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-09T18:55:34.916678Z",
          "start_time": "2019-04-09T18:55:34.837925Z"
        },
        "id": "cAlpHkcYiRcc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pair_batch = voc.batchSent2VecData([random.choice(np.arange(len(pairs))) for _ in range(64)],pairs)\n",
        "\n",
        "input_variable, lengths, target_variable, mask, max_target_len,emo_in,emo_out = batches\n",
        "# loss = train(input_variable,lengths, target_variable, mask, max_target_len,encoder,\n",
        "#                       decoder,embedding,encoder_optimizer, decoder_optimizer, batch_size, clip)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BOPy6FLMpwwA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_outputs, encoder_hidden = encoder(input_variable, lengths)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6UnFIlmWrNQz",
        "colab_type": "code",
        "outputId": "e96380d8-502c-41ef-c522-b05ef6cfe8f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # Teacher forcing: next input is current target\n",
        "decoder_input = target_variable[t].view(1, -1)\n",
        "            # Calculate and accumulate loss\n",
        "mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([22, 64, 500])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-09T18:57:21.049018Z",
          "start_time": "2019-04-09T18:57:20.900608Z"
        },
        "id": "_3Zg0UjMiRce",
        "colab_type": "code",
        "outputId": "cd56c859-9c86-4042-ea94-a4acc6d3e405",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "input_variable = input_variable.to(device)\n",
        "lengths = lengths.to(device)\n",
        "# encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "gru = nn.GRU(hidden_size, hidden_size,2,dropout=0, bidirectional=True)\n",
        "hidden=None\n",
        "embedded = embedding(input_variable)\n",
        "torch.as_tensor(lengths, dtype=torch.int64)\n",
        "# packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, lengths)\n",
        "# packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "# outputs, hidden = gru(packed, hidden)\n",
        "# outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "# outputs = outputs[:, :, :hidden_size] + outputs[:, : ,hidden_size:]\n",
        "embedded.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([22, 64, 500])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "Sf0kqimNdaYD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loss"
      ]
    },
    {
      "metadata": {
        "id": "gszDWLLidbnY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "76b0c716-6c58-4844-8358-ff4261557b84"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# mask_loss,nTotal = maskNLLLoss(decoder_output,input_emotion,target_variable[t],mask[t],decoder)\n",
        "# (inp,emotion,target,mask,decoder):\n",
        "target = target_variable[1]\n",
        "nTotal = mask[1].sum()\n",
        "crossEntropy = -torch.log(torch.gather(decoder_output, 1, target.view(-1, 1)).squeeze(1))\n",
        "loss = crossEntropy.masked_select(mask[1]).mean()\n",
        "emo_loss = torch.norm(input_emotion.squeeze(),dim=1)\n",
        "emo_loss = emo_loss.masked_select(1-mask[1]).mean()\n",
        "emo_loss = 0 if math.isnan(emo_loss) else emo_loss\n",
        "loss+= emo_loss\n",
        "loss"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(10.7795, grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "metadata": {
        "id": "UtypV2IpEts8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "7a7fac5f-f625-455c-dfb5-a118d54af459"
      },
      "cell_type": "code",
      "source": [
        "1-mask[1]"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "metadata": {
        "id": "u8Oa126_djuo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Debug Beam Search"
      ]
    },
    {
      "metadata": {
        "id": "Nj-PjEnhsJ9B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "outputId": "9ddb19f3-bff3-4734-bd12-aa03f086bf2a"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# def evaluateInput(encoder,decoder,searcher,voc,num_output=1,max_length=MAX_LENGTH):\n",
        "max_length=MAX_LENGTH\n",
        "\n",
        "searcher = MyTopKDecoder(encoder,decoder,3)\n",
        "indexes_batch = [voc.indexesFromSentence(input_sentence,normalize=False)]\n",
        "lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "input_batch = input_batch.to(device)\n",
        "lengths = lengths.to(device)\n",
        "e = 1\n",
        "tmp = searcher(input_batch,e,lengths,3,max_length)\n",
        "\n",
        "for i in range(min(len(tmp),3)):\n",
        "    if voc.version == \"bpemb\":\n",
        "        padding = [0,1,2]\n",
        "        words = [token.item() for token in tmp[i]]\n",
        "        filtered = filter(lambda x: True if x not in padding else False, words)\n",
        "        print('{}: '.format(idx2emo[e]), voc.bpemb.decode_ids(list(filtered)))\n",
        "    else:\n",
        "        decoded_words = [voc.index2word[token.item()] for token in tmp[i]]\n",
        "        decoded_words[:] = [x for x in decoded_words if not (x == 'EOS' or x == 'PAD' or x == 'SOS')]\n",
        "        print('{}: '.format(idx2emo[e]), ' '.join(decoded_words))\n",
        "\n",
        "               "
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-12de71469b0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-e5f9d002c47f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_seq, target_emotion, input_length, num_output, max_length)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mall_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0memotion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_emotion\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbeam_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-05b2a1c1ae02>\u001b[0m in \u001b[0;36mbeam_decode\u001b[0;34m(decoder, decoder_emotion, decoder_hidden, encoder_output, topk, num_output, debug)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m#         torch.unsqueeze(indexes[0][new_k], 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mnextnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mword_past\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnew_k\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeam_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mdecoded_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_k\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'BeamSearchNode' object has no attribute 'past'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "YxEJ0_PJu1L1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Beam Decode"
      ]
    },
    {
      "metadata": {
        "id": "okKefl8UeEb1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_output, encoder_hidden = encoder(input_batch, lengths)\n",
        "decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
        "all_scores = torch.zeros([0], device=device)\n",
        "decoder_emotion = torch.LongTensor([e]).to(device)\n",
        "# beam_decode2(decoder, emotion, decoder_hidden, encoder_output=encoder_output,topk=3,debug=True)     \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8vYayFgn0fDP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6aa347a8-f8bd-4fc2-aa70-817021b660ce"
      },
      "cell_type": "code",
      "source": [
        "node = BeamSearchNode(decoder_hidden, decoder_emotion, None, decoder_input, 0, 1)\n",
        "-node.eval()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "id": "NPTaQjdDPOiM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "min_sentence_length = 15\n",
        "\n",
        "def valid_node(n,wordidx):\n",
        "    word = voc.index2word[wordidx.item()] \n",
        "    eval_1 = not word in sent_breaker\n",
        "    eval_2 = n.leng > min_sentence_length\n",
        "#     eval_2 = decoded_t.item() in word_past\n",
        "#     eval_3 = len(voc.bpemb.decode_ids([decoded_t.item()]))>similar_word_len\n",
        "    \n",
        "    return eval_1 and eval_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "htDqHzIjgBew",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Beam_width 是每次选几个， Topk是最终留几个\n",
        "sent_breaker = ['.','.','!','?',':','EOS']\n",
        "endnodes = []\n",
        "topk = 10\n",
        "decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
        "node = BeamSearchNode(decoder_hidden, decoder_emotion, None, decoder_input, 0, 1)\n",
        "nodes = PriorityQueue()\n",
        "nodes.put((-node.eval(), node))\n",
        "qsize = 1\n",
        "while True:\n",
        "        # Break condition\n",
        "        if qsize > max_qsize or nodes.empty(): break\n",
        "        \n",
        "        # Fetch\n",
        "        score, n = nodes.get()\n",
        "        decoder_input = n.wordid\n",
        "        decoder_hidden = n.h\n",
        "        \n",
        "        # If we reach end of sentence, put it on endnodes list\n",
        "        if n.wordid.item() == EOS_token and n.prevNode != None:\n",
        "            endnodes.append((score, n))\n",
        "            # if we reached maximum # of sentences required\n",
        "            if len(endnodes) >= 50:\n",
        "                break\n",
        "                \n",
        "        # Decode one step\n",
        "        decoder_output,decoder_hidden,decoder_emotion = decoder(decoder_input,decoder_emotion,decoder_hidden,encoder_output)\n",
        "        log_prob, indexes = torch.topk(decoder_output, beam_width)\n",
        "        \n",
        "        \n",
        "        # Extend Queue\n",
        "        nextnodes = []\n",
        "#         word_past = n.past.copy()\n",
        "        for new_k in range(beam_width):\n",
        "            decoded_t = indexes[0][new_k].view(1, -1)   \n",
        "            \n",
        "#             eval_1 = voc.index2word[decoded_t.item()] in sent_breaker\n",
        "#             eval_2 = decoded_t.item() in word_past\n",
        "#             eval_3 = False\n",
        "            \n",
        "            if valid_node(n,decoded_t):\n",
        "#                 print(\"current word is {}\".format(voc.index2word[decoded_t.item()]),eval_1,eval_2,eval_3)\n",
        "                log_p = log_prob[0][new_k].item()\n",
        "                word_past.append(decoded_t.item())\n",
        "                if len(word_past)>max_past_word:\n",
        "                    word_past = word_past[-max_past_word:]\n",
        "                node = BeamSearchNode(decoder_hidden,decoder_emotion, n,decoded_t,n.logp + log_p, n.leng + 1,word_past)\n",
        "                score = -node.eval()\n",
        "#                 \n",
        "                nextnodes.append((score, node))\n",
        "        # put them into queue\n",
        "        for i in range(len(nextnodes)):\n",
        "            score, nn = nextnodes[i]\n",
        "#             print(score,voc.index2word[nn.wordid.item()])\n",
        "            nodes.put((score, nn))\n",
        "            # increase qsize\n",
        "        qsize += len(nextnodes) - 1\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rZZJzLS1gZT1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if len(endnodes) == 0:\n",
        "    endnodes = [nodes.get() for _ in range(min(200,nodes.qsize()))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OMNmdgWfvVCJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2c88d6bb-b10b-4772-8ef6-82f197623723"
      },
      "cell_type": "code",
      "source": [
        "len(endnodes)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "metadata": {
        "id": "H-a-rH8ThXwr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "utterances = []\n",
        "#     print(len(endnodes))\n",
        "for score, n in sorted(endnodes, key=operator.itemgetter(0)):\n",
        "\n",
        "    utterance = []\n",
        "    utterance.append(n.wordid)\n",
        "    # back trace\n",
        "    while n.prevNode != None:\n",
        "        n = n.prevNode\n",
        "        utterance.append(n.wordid)\n",
        "\n",
        "    utterance = utterance[::-1]\n",
        "    print_sentence(utterance)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c5QSkhJAvBHW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def print_sentence(utterance):\n",
        "    decoded_words = [voc.index2word[token.item()] for token in utterance]\n",
        "    decoded_words[:] = [x for x in decoded_words if not (x == 'EOS' or x == 'PAD' or x == 'SOS')]\n",
        "    print('{}: '.format(idx2emo[e]), ' '.join(decoded_words))\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rj6ygSsqea0t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "79ee19bc-29b3-4050-83ad-ba18ee4ae91e"
      },
      "cell_type": "code",
      "source": [
        "beam_width = 10\n",
        "max_qsize = 1000\n",
        "max_past_word = 20\n",
        "min_sentence_length = 10\n",
        "similar_word_len = 2\n",
        "\n",
        "\n",
        "def beam_decode2(decoder,decoder_emotion,decoder_hidden,encoder_output,topk,debug=False):\n",
        "    sent_breaker = ['.','.','!','?',':','EOS']\n",
        "    endnodes = []\n",
        "    decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
        "    \n",
        "    # starting node -  hidden vector, previous node, word id, logp, length\n",
        "    node = BeamSearchNode(decoder_hidden, decoder_emotion, None, decoder_input, 0, 1)\n",
        "    nodes = PriorityQueue()\n",
        "    nodes.put((-node.eval(), node))\n",
        "    qsize = 1\n",
        "    \n",
        "    \n",
        "    \n",
        "    # start beam search\n",
        "    while True:\n",
        "        # give up when decoding takes too long\n",
        "        if qsize > max_qsize or nodes.empty(): break\n",
        "        \n",
        "        # fetch the best node\n",
        "        score, n = nodes.get()\n",
        "        decoder_input = n.wordid\n",
        "        decoder_hidden = n.h\n",
        "        if n.wordid.item() == EOS_token and n.prevNode != None:\n",
        "            endnodes.append((score, n))\n",
        "            # if we reached maximum # of sentences required\n",
        "            if len(endnodes) >= 5*topk:\n",
        "                break\n",
        "        # decode one step using decoder\n",
        "        decoder_output,decoder_hidden,decoder_emotion = decoder(decoder_input,decoder_emotion,decoder_hidden,encoder_output)\n",
        "        log_prob, indexes = torch.topk(decoder_output, beam_width)\n",
        "\n",
        "        nextnodes = []\n",
        "        word_past = n.past.copy()\n",
        "        for new_k in range(beam_width):\n",
        "            decoded_t = indexes[0][new_k].view(1, -1)          \n",
        "            eval_1 = voc.index2word[decoded_t.item()] in sent_breaker\n",
        "            eval_2 = decoded_t.item() in word_past\n",
        "            eval_3 = False\n",
        "#             eval_3 = len(voc.bpemb.decode_ids([decoded_t.item()]))>similar_word_len\n",
        "            if (eval_1 and n.leng < min_sentence_length) or (eval_2 and eval_3):\n",
        "                continue\n",
        "            else:   \n",
        "#                 print(\"current word is {}\".format(voc.bpemb.decode_ids([decoded_t.item()])))\n",
        "                log_p = log_prob[0][new_k].item()\n",
        "                word_past.append(decoded_t.item())\n",
        "                if len(word_past)>max_past_word:\n",
        "                    word_past = word_past[-max_past_word:]\n",
        "                node = BeamSearchNode(decoder_hidden,decoder_emotion, n,decoded_t,n.logp + log_p, n.leng + 1,word_past)\n",
        "                score = -node.eval()\n",
        "                nextnodes.append((score, node))\n",
        "        # put them into queue\n",
        "        for i in range(len(nextnodes)):\n",
        "            score, nn = nextnodes[i]\n",
        "            nodes.put((score, nn))\n",
        "            # increase qsize\n",
        "        qsize += len(nextnodes) - 1\n",
        "    \n",
        "    # choose nbest paths, back trace them\n",
        "    if len(endnodes) == 0:\n",
        "        endnodes = [nodes.get() for _ in range(min(100,nodes.qsize()))]\n",
        "    utterances = []\n",
        "#     print(len(endnodes))\n",
        "    for score, n in sorted(endnodes, key=operator.itemgetter(0)):\n",
        "        if n.leng<min_sentence_length:\n",
        "            continue\n",
        "        utterance = []\n",
        "        utterance.append(n.wordid)\n",
        "        while n.prevNode != None:\n",
        "            n = n.prevNode\n",
        "            utterance.append(n.wordid)\n",
        "        \n",
        "        utterance = utterance[::-1]\n",
        "        utterances.append(utterance)\n",
        "\n",
        "\n",
        "    return utterances"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 205
        }
      ]
    },
    {
      "metadata": {
        "id": "E7aKByumecmq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "beam_width = 10\n",
        "max_qsize = 1000\n",
        "min_sentence_length = 10\n",
        "similar_word_len = 2\n",
        "class Searcher(nn.Module):\n",
        "    def __init__(self, encoder,decoder,k=1,debug=False):\n",
        "        super(Searcher, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.k = k\n",
        "        self.debug = debug\n",
        "    def forward(self, input_seq,target_emotion,input_length,max_length=MAX_LENGTH):\n",
        "        # Forward input through encoder model\n",
        "        encoder_output, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
        "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "        # Initialize decoder input with SOS_token\n",
        "        \n",
        "        # Initialize tensors to append decoded words to\n",
        "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
        "        all_scores = torch.zeros([0], device=device)\n",
        "        decoder_emotion = torch.LongTensor([target_emotion]).to(device)\n",
        "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
        "\n",
        "        if self.k == 1:\n",
        "            for _ in range(max_length):\n",
        "                # Forward pass through decoder\n",
        "                decoder_output, decoder_hidden, decoder_emotion = self.decoder(decoder_input,decoder_emotion, decoder_hidden, encoder_output)\n",
        "                # Obtain most likely word token and its softmax score\n",
        "                decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "                # Record token and score\n",
        "                all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "                all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "                # Prepare current token to be next decoder input (add a dimension)\n",
        "                decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "            # Return collections of word tokens and scores\n",
        "            return all_tokens, all_scores\n",
        "        else:\n",
        "            return self.beam_decode(decoder_emotion,decoder_hidden,encoder_output)  \n",
        "          \n",
        "    def beam_decode(self,decoder_emotion,decoder_hidden,encoder_output,debug=False):\n",
        "        sent_breaker = ['.','.','!','?',':','EOS']\n",
        "        endnodes = []\n",
        "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
        "\n",
        "        # starting node -  hidden vector, previous node, word id, logp, length\n",
        "        node = BeamSearchNode(decoder_hidden, decoder_emotion, None, decoder_input, 0, 1)\n",
        "        nodes = PriorityQueue()\n",
        "        nodes.put((-node.eval(), node))\n",
        "        qsize = 1\n",
        "\n",
        "        # start beam search\n",
        "        while True:\n",
        "            # give up when decoding takes too long\n",
        "            if qsize > max_qsize or nodes.empty(): break\n",
        "\n",
        "            # fetch the best node\n",
        "            score,n = nodes.get()\n",
        "            decoder_input = n.wordid\n",
        "            decoder_hidden = n.h\n",
        "            if n.wordid.item() == EOS_token and n.prevNode != None:\n",
        "                endnodes.append((score, n))\n",
        "                # if we reached maximum # of sentences required\n",
        "                if len(endnodes) >= topk:\n",
        "                    break\n",
        "            # decode one step using decoder\n",
        "            decoder_output,decoder_hidden,decoder_emotion = self.decoder(decoder_input,decoder_emotion,decoder_hidden,encoder_output)\n",
        "            log_prob, indexes = torch.topk(decoder_output, beam_width)\n",
        "\n",
        "            nextnodes = []\n",
        "#             word_past = n.past.copy()\n",
        "            for new_k in range(beam_width):\n",
        "                decoded_t = indexes[0][new_k].view(1, -1)          \n",
        "                #                 eval_1 = voc.index2word[decoded_t.item()] in sent_breaker\n",
        "                #                 eval_2 = decoded_t.item() in word_past\n",
        "                #                 eval_3 = False\n",
        "                #             eval_3 = len(voc.bpemb.decode_ids([decoded_t.item()]))>similar_word_len\n",
        "                #                 if (eval_1 and n.leng < min_sentence_length) or (eval_2 and eval_3):\n",
        "                #                     continue\n",
        "                #                 else:   \n",
        "                #                 print(\"current word is {}\".format(voc.bpemb.decode_ids([decoded_t.item()])))\n",
        "                log_p = log_prob[0][new_k].item()\n",
        "                #                   word_past.append(decoded_t.item())\n",
        "                #                   if len(word_past)>max_past_word:\n",
        "                #                       word_past = word_past[-max_past_word:]\n",
        "                node = BeamSearchNode(decoder_hidden,decoder_emotion, n,decoded_t,n.logp + log_p, n.leng + 1)\n",
        "                score = -node.eval()\n",
        "                nextnodes.append((score, node))\n",
        "            # put them into queue\n",
        "            for i in range(len(nextnodes)):\n",
        "                score, nn = nextnodes[i]\n",
        "                nodes.put((score, nn))\n",
        "#                 print(score,nn.item())\n",
        "                # increase qsize\n",
        "            qsize += len(nextnodes) - 1\n",
        "\n",
        "        # choose nbest paths, back trace them\n",
        "        if len(endnodes) == 0:\n",
        "            endnodes = [nodes.get() for _ in range(min(100,nodes.qsize()))]\n",
        "        utterances = []\n",
        "    #     print(len(endnodes))\n",
        "        for score, n in sorted(endnodes, key=operator.itemgetter(0)):\n",
        "            if n.leng<min_sentence_length:\n",
        "                continue\n",
        "            utterance = []\n",
        "            utterance.append(n.wordid)\n",
        "            while n.prevNode != None:\n",
        "                n = n.prevNode\n",
        "                utterance.append(n.wordid)\n",
        "\n",
        "            utterance = utterance[::-1]\n",
        "            utterances.append(utterance)\n",
        "\n",
        "        return utterances\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}